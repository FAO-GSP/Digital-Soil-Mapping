image = NDVI,
scale= res,
region = region,
via = "drive"
)
evir <- ee_as_raster(
image = EVI,
scale= res,
region = region,
via = "drive"
)
writeRaster(ndvir, '01-Data/covs/NDVI.tif', overwrite=T)
writeRaster(evir, '01-Data/covs/EVI.tif', overwrite=T)
image <- ee$ImageCollection("MODIS/061/MOD11A1") %>%
ee$ImageCollection$filterDate("2018-01-01", "2018-12-31") %>%
ee$ImageCollection$select("LST_Day_1km")%>%
ee$ImageCollection$filterBounds(region)%>%
ee$ImageCollection$toBands()
d_T = image$subtract(273.15)
sd_d_T = d_T$reduce(ee$Reducer$stdDev())
sd_d_T = sd_d_T$resample('bilinear')$reproject(
crs= crs,
scale= res)
sd_d_Tr <- ee_as_raster(
image = sd_d_T,
scale= res,
region = region,
via = "drive"
)
writeRaster(sd_d_Tr, '01-Data/covs/sd_d_Tr.tif', overwrite=T)
# Landsat bands mean and sd ----
image <- ee$ImageCollection("LANDSAT/LC08/C02/T1_RT") %>%
ee$ImageCollection$filterDate("2018-01-01", "2018-12-31") %>%
ee$ImageCollection$select(c("B4"))%>%
ee$ImageCollection$filterBounds(region)%>%
ee$ImageCollection$toBands()
land_red = image$reduce(ee$Reducer$stdDev())
land_red = land_red$resample('bilinear')$reproject(
crs= crs,
scale= res)
land_sd_red <- ee_as_raster(
image = land_red,
scale= res,
region = region,
via = "drive"
)
writeRaster(land_sd_red, '01-Data/covs/land_sd_red.tif', overwrite=T)
image <- ee$ImageCollection("LANDSAT/LC08/C02/T1_RT") %>%
ee$ImageCollection$filterDate("2018-01-01", "2018-12-31") %>%
ee$ImageCollection$select(c("B5"))%>%
ee$ImageCollection$filterBounds(region)%>%
ee$ImageCollection$toBands()
land_nir = image$reduce(ee$Reducer$stdDev())
land_nir = land_nir$resample('bilinear')$reproject(
crs= crs,
scale= res)
land_nirr <- ee_as_raster(
image = land_nir,
scale= res,
region = region,
via = "drive"
)
writeRaster(land_nirr, '01-Data/covs/land_sd_nir.tif', overwrite=T)
image <- ee$Image("OpenLandMap/SOL/SOL_WATERCONTENT-33KPA_USDA-4B1C_M/v01") %>%
ee$Image$select(c('b0','b10','b30'))%>%
ee$Image$clip(region)
soil_wt = image$resample('bilinear')$reproject(
crs= crs,
scale= res)
soil_wtr <- ee_as_raster(
image = soil_wt,
scale= res,
region = region,
via = "drive"
)
#Harmonize to 0-30 depth with a weighted average
WeightedAverage<-function(r){return(r[[1]]*(1/30)+r[[2]]*(9/30)+r[[3]]*(20/30))}
soil_wtr<-overlay(soil_wtr,fun=WeightedAverage)
writeRaster(soil_wtr, '01-Data/covs/soil_wtr.tif', overwrite=T)
image <- ee$Image("OpenLandMap/PNV/PNV_FAPAR_PROBA-V_D/v01") %>%
ee$Image$select(c('jan','feb','mar','apr','may','jun','jul',
'aug','sep','oct','nov','dec' ))%>%
ee$Image$clip(region)
fapar = image$resample('bilinear')$reproject(
crs= crs,
scale= res)
fapar=fapar$reduce(ee$Reducer$mean())
faparr <- ee_as_raster(
image = fapar,
scale= res,
region = region,
via = "drive"
)
writeRaster(faparr, '01-Data/covs/faparr_mean.tif', overwrite=T)
# Multi-Scale Topographic Position Index ----
image <- ee$Image("CSP/ERGo/1_0/Global/ALOS_mTPI") %>%
ee$Image$select('AVE')%>%
ee$Image$clip(region)
top_pos = image$resample('bilinear')$reproject(
crs= crs,
scale= res)
top_posr <- ee_as_raster(
image = top_pos,
scale= res,
region = region,
via = "drive"
)
writeRaster(top_posr, '01-Data/covs/top_posr.tif', overwrite=T)
# Clip and store covariates in working directory
AOI <- vect(AOI)
files <- list.files(path = output_dir, pattern = resOLM, full.names = T)
covs <- stack(files)
covs <- rast(covs)
covs <- crop(covs, AOI)
covs <- mask(covs, AOI)
#Use one rgee raster to harmonize the covs
rgee <-rast('01-Data/covs/Prr.tif')
covs <- resample(covs, rgee)
# Export PCs based on Principle Component Analysis
#Convert zenodo layers to gee object
zenodo_covs <- raster_as_ee(
x = covs,
overwrite = TRUE,
assetId = assetId,
bucket = "rgee_dev"
)
?raster_as_ee
# Export PCs based on Principle Component Analysis
#Convert zenodo layers to gee object
zenodo_covs <- raster_as_ee(
x = covs,
overwrite = TRUE
)
View(Prr_wet)
covs
covs <- raster(covs)
# Export PCs based on Principle Component Analysis
#Convert zenodo layers to gee object
zenodo_covs <- raster_as_ee(
x = covs,
overwrite = TRUE,
assetId = assetId,
bucket = "rgee_dev"
)
install.packages('googleCloudStorageR')
library(googleCloudStorageR)
# Export PCs based on Principle Component Analysis
#Convert zenodo layers to gee object
zenodo_covs <- raster_as_ee(
x = covs,
overwrite = TRUE,
assetId = assetId,
bucket = "rgee_dev"
)
# Mean annual temperature (daytime) ----
ee_Initialize(gcs = TRUE)
SaK_file <- "C:/Users/luottoi/Documents/secret-opus-351513-ef67b632c716.json" # PUT HERE THE FULLNAME OF YOUR SAK.
# Assign the SaK to a EE user.
ee_utils_sak_copy(
sakfile =  SaK_file,
users = c("gsprgeekey") # Unlike GD, we can use the same SaK for multiple users.
)
# Assign the SaK to a EE user.
ee_utils_sak_copy(
sakfile =  SaK_file # Unlike GD, we can use the same SaK for multiple users.
)
# Validate your SaK
ee_utils_sak_validate()
# Mean annual temperature (daytime) ----
ee_Initialize(gcs = TRUE)
# Export PCs based on Principle Component Analysis
#Convert zenodo layers to gee object
zenodo_covs <- raster_as_ee(
x = covs,
overwrite = TRUE,
assetId = assetId,
bucket = "rgee_dev"
)
# Export PCs based on Principle Component Analysis
#Convert zenodo layers to gee object
zenodo_covs <- raster_as_ee(
x = covs,
overwrite = TRUE,
bucket = "rgee_dev"
)
?raster_as_ee
assetId <- sprintf("%s/%s",ee_get_assethome(),'raster_l7')
zenodo_covs <- raster_as_ee(
x = covs,
overwrite = TRUE,
bucket = "rgee_dev",
assetId=assetId
)
zenodo_covs <- raster_as_ee(
x = covs,
overwrite = TRUE,
bucket = "gsprgeekey",
assetId=assetId
)
zenodo_covs
assetId
assetId <- sprintf("%s/%s",ee_get_assethome())
assetId <- sprintf("%s/%s",ee_get_assethome(),'')
assetId
zenodo_covs <- raster_as_ee(
x = covs,
overwrite = TRUE,
bucket = "gsprgeekey",
assetId=assetId
)
assetId <- sprintf("%s/%s",ee_get_assethome(),'zenodo')
zenodo_covs <- raster_as_ee(
x = covs,
overwrite = TRUE,
bucket = "gsprgeekey",
assetId=assetId
)
plot(covs)
# Clip and store covariates in working directory
AOI <- vect(AOI)
files <- list.files(path = output_dir, pattern = resOLM, full.names = T)
covs <- stack(files)
covs <- rast(covs)
covs <- crop(covs, AOI)
covs <- mask(covs, AOI)
#Use one rgee raster to harmonize the covs
rgee <-rast('01-Data/covs/Prr.tif')
covs <- resample(covs, rgee)
writeRaster(covs, '01-Data/covs/olm_covs.tif', overwrite=T)
covs <- stack(covs)
plot(covs)
assetId <- sprintf("%s/%s",ee_get_assethome(),'zenodo')
zenodo_covs <- raster_as_ee(
x = covs,
overwrite = TRUE,
bucket = "gsprgeekey",
assetId=assetId
)
# Stack bands
#avT,Pr,Prr_all,DEMAttributes,EVI,NDVI,sd_d_T,land_red,land_nir,soil_wt,fapar,top_pos
SG <-
avT$addBands(Pr)$addBands(Prr_all)$addBands(DEMAttributes)$addBands(EVI)
SG <-SG$addBands(NDVI)$addBands(land_red)$addBands(sd_d_T)$addBands(land_nir)
SG <-SG$addBands(fapar)$addBands(soil_wt)$addBands(top_pos)
class(SG)
metadata <- SG$propertyNames()
cat("Metadata: ", paste(metadata$getInfo(),"\n",collapse = " "))
inputBandNames <- SG$bandNames()$getInfo()
print(inputBandNames)
dimOne <- length(inputBandNames)
cat("Number of input bands:", dimOne)
# Calculate scale standardize each band to mean=0, s.d.=1. ----
scale <- SG$select("mean")$projection()$nominalScale()
cat("Nominal scale: ", scale$getInfo())
SGmean <- SG$reduceRegion(ee$Reducer$mean(),
geometry = region, scale = scale, bestEffort = TRUE)
head(SGmean$getInfo(),3) # an example of the means
SGsd <- SG$reduceRegion(ee$Reducer$stdDev(),
geometry = region, scale = scale, bestEffort = TRUE)
head(SGsd$getInfo(),3)
saveRDS(as.vector(SGmean$getInfo()), file = "./inputBandMeans.rds")
saveRDS(as.vector(SGsd$getInfo()), file = "./inputBandSDs.rds")
SGmean.img <- ee$Image$constant(SGmean$values(inputBandNames))
SGsd.img <- ee$Image$constant(SGsd$values(inputBandNames))
#Standardize
SGstd <- SG$subtract(SGmean.img)
SGstd <- SGstd$divide(SGsd.img)
SGstd.minMax <- SGstd$reduceRegion(ee$Reducer$minMax(),
geometry = region, scale = scale,
maxPixels = 1e9, bestEffort = TRUE)
minMaxNames <- names(SGstd.minMax$getInfo())
minMaxVals <- SGstd.minMax$values()$getInfo()
# per property min/max
head(data.frame(property_depth_q=minMaxNames, value=minMaxVals), 12)
# overall min/max
print(c(min(minMaxVals), max(minMaxVals)) )
# convert image to an array of pixels, for matrix calculations
# dimensions are N x P; i.e., pixels x bands
arrays <- SGstd$toArray()
# Compute the covariance of the bands within the region.
covar <- arrays$reduceRegion(
ee$Reducer$centeredCovariance(),
geometry = region, scale = scale,
maxPixels = 1e6,
bestEffort = TRUE
)
# Get the covariance result and cast to an array.
# This represents the band-to-band covariance within the region.
covarArray <- ee$Array(covar$get('array'))
# note we know the dimensions from the inputs
# Perform an eigen analysis and slice apart the values and vectors.
eigens <- covarArray$eigen()
# by removing the first axis (eigenvalues) and converting to a vector
# array$slice(axis=0, start=0, end=null, step=1)
# here we only have one axis, indexed by the PC
eigenValues <- eigens$slice(1L, 0L, 1L)
cat('Eigenvalues:')
## Eigenvalues:
print(eigenValues.vect <- unlist(eigenValues$getInfo()))
# compute and show proportional eigenvalues
eigenValuesProportion <-
round(100*(eigenValues.vect/sum(eigenValues.vect)),2)
cat('PCs percent of variance explained:')
## PCs percent of variance explained:
print(eigenValuesProportion)
plot(eigenValuesProportion, type="h", xlab="PC",
ylab="% of variance explained",
main="Standardized PCA")
evsum <- cumsum(eigenValuesProportion)
cat('PCs percent of variance explained, cumulative sum:')
## PCs percent of variance explained, cumulative sum:
print(round(evsum,1))
cat(npc95 <- which(evsum > 95)[1],
"PCs are needed to explain 95% of the variance")
saveRDS(eigenValues.vect, file = "./eigenValuesVector.rds")
# The eigenvectors (rotations); this is a PxP matrix with eigenvectors in rows.
eigenVectors <- eigens$slice(1L, 1L)
# show the first few rotations
cat('Eigenvectors 1--3:')
## Eigenvectors 1--3:
print(data.frame(band=inputBandNames,
rotation1 = eigenVectors$getInfo()[[1]],
rotation2 = eigenVectors$getInfo()[[2]],
rotation3 = eigenVectors$getInfo()[[3]]
))
#export the eigenvectors ----
eVm <- matrix(unlist(eigenVectors$getInfo()), byrow = TRUE, nrow = dimOne)
saveRDS(eVm, file = "./eigenvectorMatrix.rds")
arrayImage <- arrays$toArray(1L)
PCsMatrix <- ee$Image(eigenVectors)$matrixMultiply(arrayImage)
# arrayProject: "Projects the array in each pixel to a lower dimensional #
#space by specifying the axes to retain"
# arrayFlatten: "Converts a single band image of equal-shape
# multidimensional pixels
# to an image of scalar pixels,
# with one band for each element of the array."
PCbandNames <- as.list(paste0('PC', seq(1L:dimOne)))
PCs <- PCsMatrix$arrayProject(list(0L))$arrayFlatten(
list(PCbandNames)
)
PCs95 <- PCs$select(0L:(npc95-1L)) # indexing starts from 0
PCs95$bandNames()$getInfo()
# Export PCs as raster stack ----
PCAs_covs <- ee_as_raster(
image = PCs95,
scale= res,
region = region,
via = "drive"
)
writeRaster(PCAs_covs, '02-Outputs/PCA_covariates.tif', overwrite= T)
#Empty environment and cache
rm(list = ls());
gc()
# Working directory
wd <- 'C:/Users/luottoi/Documents/GitHub/Digital-Soil-Mapping'
#List of soil attributes prepared in script #2
soilatt <- c('SOC','clay', 'pH')
#
#
#######################################################
# Set working directory
setwd(wd)
############################### Prapare the final table for modelling (regression matrix)
library(raster)
library(sp)
library(caret)
library(Metrics)
library(quantregForest)
library(snow)
#Load PCs
covs_pc <- stack('02-Outputs/PCA_covariates.tif')
for(i in unique(soilatt)){
# Load the covariates stack. It was was prepared in the 'data_preparation_covariates' script
load(file = paste0("02-Outputs/", i,"_covariates.RData"))
names(covs)
# Load the processed data for digital soil mapping. This table was prepared in the 'data_preparation_profiles' script
dat <- read.csv(paste0("02-Outputs/",i,"_dat_train.csv"))
names(dat)
# extract values from covariates to the soil points
coordinates(dat) <- ~ X + Y
dat <- extract(x = covs, y = dat, sp = TRUE)
summary(dat)
# Remove NA values
dat<-as.data.frame(dat)
dat <- dat[complete.cases(dat),]
str(dat)
# LandCover and soilmap are categorical variables, they need to be 'factor' type
dat$soilmap <- as.factor(dat$soilmap)
str(dat)
coordinates(dat) <- ~ X + Y
# Generate an empty dataframe
validation <- data.frame(rmse=numeric(), r2=numeric())
fm = as.formula(paste(i," ~", paste0(names(covs),
collapse = "+")))
fm
ctrl <- trainControl(method = "cv", savePred=T)
# Sensitivity to the dataset
# Start a loop with 10 model realizations
pred <- stack()
for (j in 1:10){
# We will build 10 models using random samples of 25%
smp_size <- floor(0.25 * nrow(dat))
train_ind <- sample(seq_len(nrow(dat)), size = smp_size)
train <- dat[train_ind, ]
test <- dat[-train_ind, ]
modn <- train(fm, data=train@data, method = "rf",
trControl = ctrl)
pred_cv <-predict(covs, modn)
pred <- stack(pred, pred_cv)
test$pred <- extract(pred[[j]], test)
# Store the results in a dataframe
validation[j, 1] <- rmse(test[[i]], test$pred)
validation[j, 2] <- cor(test[[i]], test$pred)^2
}
# The sensitivity map is the dispersion of all individual models
sensitivity <- calc(pred, sd)
plot(sensitivity, col=rev(topo.colors(10)),
main='Sensitivity based on 10 realizations using 25% samples')
summary(validation)
#generate mtry variable using caret package
rfmodel<- train(fm, data=dat@data, method = "rf", trControl = ctrl,
importance=TRUE)
# run the quantile regression forest algorithm
model <- quantregForest(y=dat@data[[i]], x=dat@data[,names(covs)],
ntree=500, keep.inbag=TRUE,
mtry = as.numeric(rfmodel$bestTune))
# Define number of cores to use
beginCluster()
## 8 cores detected, using 7
# Estimate model uncertainty
unc <- clusterR(covs, predict, args=list(model=model,what=sd))
# OCS prediction based in all available data
mean <- clusterR(covs, predict, args=list(model=model,what=mean))
# The total uncertainty is the sum of sensitivity and model
# uncertainty
unc <- unc + sensitivity
# Express the uncertainty in percent % (divide by the mean)
Total_unc_Percent <- unc/mean*100
endCluster()
# Plot both maps (the predicted OCS and associated uncertainty)
plot(mean, main='based in all data')
plot(Total_unc_Percent, main='Total uncertainty %')
writeRaster(mean, paste0('02-Outputs/Final Maps/',i,'_RF.tif'), overwrite=TRUE)
writeRaster(unc, paste0('02-Outputs/Final Maps/',i,'_RF_sd.tif'), overwrite=TRUE)
# Run analysis with PCs
# Load the processed data for digital soil mapping. This table was prepared in the 'data_preparation_profiles' script
dat <- read.csv(paste0("02-Outputs/",i,"_dat_train.csv"))
names(dat)
# extract values from covariates to the soil points
coordinates(dat) <- ~ X + Y
dat <- extract(x = covs_pc, y = dat, sp = TRUE)
summary(dat)
# Remove NA values
dat<-as.data.frame(dat)
dat <- dat[complete.cases(dat),]
str(dat)
coordinates(dat) <- ~ X + Y
# Generate an empty dataframe
validation <- data.frame(rmse=numeric(), r2=numeric())
fm = as.formula(paste(i," ~", paste0(names(covs_pc),
collapse = "+")))
fm
ctrl <- trainControl(method = "cv", savePred=T)
# Sensitivity to the dataset
# Start a loop with 10 model realizations
pred <- stack()
for (j in 1:10){
# We will build 10 models using random samples of 25%
smp_size <- floor(0.25 * nrow(dat))
train_ind <- sample(seq_len(nrow(dat)), size = smp_size)
train <- dat[train_ind, ]
test <- dat[-train_ind, ]
modn <- train(fm, data=train@data, method = "rf",
trControl = ctrl)
pred_cv <-predict(covs, modn)
pred <- stack(pred, pred_cv)
test$pred <- extract(pred[[j]], test)
# Store the results in a dataframe
validation[j, 1] <- rmse(test[[i]], test$pred)
validation[j, 2] <- cor(test[[i]], test$pred)^2
}
# The sensitivity map is the dispersion of all individual models
sensitivity <- calc(pred, sd)
plot(sensitivity, col=rev(topo.colors(10)),
main='Sensitivity based on 10 realizations using 25% samples')
summary(validation)
#generate mtry variable using caret package
rfmodel<- train(fm, data=dat@data, method = "rf", trControl = ctrl,
importance=TRUE)
# run the quantile regression forest algorithm
model <- quantregForest(y=dat@data[[i]], x=dat@data[,names(covs)],
ntree=500, keep.inbag=TRUE,
mtry = as.numeric(rfmodel$bestTune))
# Define number of cores to use
beginCluster()
## 8 cores detected, using 7
# Estimate model uncertainty
unc <- clusterR(covs, predict, args=list(model=model,what=sd))
# OCS prediction based in all available data
mean <- clusterR(covs, predict, args=list(model=model,what=mean))
# The total uncertainty is the sum of sensitivity and model
# uncertainty
unc <- unc + sensitivity
# Express the uncertainty in percent % (divide by the mean)
Total_unc_Percent <- unc/mean*100
endCluster()
# Plot both maps (the predicted OCS and associated uncertainty)
plot(mean, main='based in all data')
plot(Total_unc_Percent, main='Total uncertainty %')
writeRaster(mean, paste0('02-Outputs/Final Maps/',i,'_RF_PCA.tif'), overwrite=TRUE)
writeRaster(unc, paste0('02-Outputs/Final Maps/',i,'_RF_sd_PCA.tif'), overwrite=TRUE)
}
