# Select the best fitting pedotransfer function using subset of data that has measured BLD
BD_test <- dat[is.na(dat$BLD)==FALSE,]
estimateBD <- function(SOC, method="Saini1996"){
OM <- SOC * 1.724
if(method=="Saini1996"){BD <- 1.62 - 0.06 * OM}
if(method=="Drew1973"){BD <- 1 / (0.6268 + 0.0361 * OM)}
if(method=="Jeffrey1979"){BD <- 1.482 - 0.6786 * (log(OM))}
if(method=="Grigal1989"){BD <- 0.669 + 0.941 * exp(1)^(-0.06 * OM)}
if(method=="Adams1973"){BD <- 100 / (OM /0.244 + (100 - OM)/2.65)}
if(method=="Honeyset_Ratkowsky1989"){BD <- 1/(0.564 + 0.0556 * OM)}
return(BD)
}
# Estimate BLD for a subset using the pedotransfer functions
BD_test$Saini <- estimateBD(BD_test$SOC, method="Saini1996")
BD_test$Drew <- estimateBD(BD_test$SOC, method="Drew1973")
BD_test$Jeffrey <- estimateBD(BD_test$SOC, method="Jeffrey1979")
BD_test$Grigal <- estimateBD(BD_test$SOC, method="Grigal1989")
BD_test$Adams <- estimateBD(BD_test$SOC, method="Adams1973")
BD_test$Honeyset_Ratkowsky <- estimateBD(BD_test$SOC, method="Honeyset_Ratkowsky1989")
# Compare results
# Observed values:
summary(BD_test$BLD)
# Predicted values:
summary(BD_test$Saini)
summary(BD_test$Drew)
summary(BD_test$Jeffrey)
summary(BD_test$Grigal)
summary(BD_test$Adams)
summary(BD_test$Honeyset_Ratkowsky)
# Compare data distributions for observed and predicted BLD
plot(density(BD_test$BLD),type="l",col="black", ylim=c(0,5), lwd=2, main="Bulk Density Pedotransfer Functions")
lines(density(BD_test$Saini),col="green", lwd=2)
lines(density(BD_test$Drew),col="red", lwd=2)
lines(density(BD_test$Jeffrey),col="cyan", lwd=2)
lines(density(BD_test$Grigal),col="orange", lwd=2)
lines(density(BD_test$Adams),col="magenta", lwd=2)
lines(density(BD_test$Honeyset_Ratkowsky),col="blue", lwd=2)
legend("topleft",legend = c("Original", "Saini", "Drew", "Jeffrey", "Grigal", "Adams",
"Honeyset_Ratkowsky"), fill=c("black", "green", "red", "cyan",
"orange","magenta", "blue"))
# Plot the Selected function again
plot(density(BD_test$BLD),type="l",col="black", ylim=c(0,3.5), lwd=2, main="Bulk Density Selected Function")
lines(density(BD_test$Honeyset_Ratkowsky),col="blue", lwd=2)
legend("topleft",legend = c("Original", "Honeyset_Ratkowsky"), fill=c("black", "blue"))
# Estimate BLD for the missing points with the selected pedotransfer function
dat$BLD[is.na(dat$BLD)] <- estimateBD(dat[is.na(dat$BLD),]$SOC, method="Honeyset_Ratkowsky1989")
# Explore the results
summary(dat$BLD)
plot(density(BD_test$BLD),type="l",col="black", ylim=c(0,3.5), lwd=2, main="Bulk Density Gap-Filling")
lines(density(dat$BLD),col="green", lwd=2)
legend("topleft",legend = c("Original", "Original+Estimated"), fill=c("black", "green"))
################################ Calculating SOC stocks for standard topsoil depth 0-30cm #######################
dat <-
dat[, c("id","top","bottom","SOC","BLD","CRF","X",  "Y", "soil")]
#equal area spline SOC
SOC <-mpspline(dat, 'SOC', d = c(0,30))
temp <- data.frame()
for (i in names(SOC)){
t2 <- cbind(SOC[[i]]$id,SOC[[i]]$est_dcm[1])
temp <- rbind(temp,t2)
}
SOC <- temp
#equal area spline BLD
BLD <-mpspline(dat, 'BLD', d = c(0,30))
temp <- data.frame()
for (i in names(BLD)){
t2 <- cbind(BLD[[i]]$id,BLD[[i]]$est_dcm[1])
temp <- rbind(temp,t2)
}
BLD <- temp
#equal area spline CRF
CRFVOL <-mpspline(dat, 'CRF', d = c(0,30))
temp <- data.frame()
for (i in names(CRFVOL)){
t2 <- cbind(CRFVOL[[i]]$id,CRFVOL[[i]]$est_dcm[1])
temp <- rbind(temp,t2)
}
CRFVOL <- temp
##Since we are predicting SOC we don't need Profiles without SOC data
# keed only unique ids
dat <-dat[,c("id","X",  "Y", "soil") ]
dat <-dat[!duplicated(dat$id),]
#Rename columns of mpspline output
colnames(SOC) <- c('id', 'SOC')
colnames(BLD) <- c('id', 'BLD')
colnames(CRFVOL) <- c('id', 'CRF')
#Merge the harmonized SOC output with the data
dat <- merge(SOC, dat[,c("id","X",  "Y", "soil") ], by='id', all.x=T)
# We are mapping SOC therefore we exclude rows with NA for the SOC column
dat <- dat[complete.cases(dat$SOC),]
dat <- dat[dat$SOC>0,]
#Finally merge the other attributes and set to numeric
dat <- merge(dat, BLD, by='id', all.x=T )
dat <- merge(dat, CRFVOL, by='id', all.x=T )
dat$SOC <- as.numeric(dat$SOC)
dat$BLD <- as.numeric(dat$BLD)
dat$CRF <- as.numeric(dat$CRF)
summary(dat)
# Estimate Organic Carbon Stock
# SOC must be in g/kg (% * 10)
# BLD in kg/m3
# CRF in percentage
#Create OCSKGM function
OCSKGM <-function (ORCDRC, BLD = 1400, CRFVOL = 0, HSIZE, ORCDRC.sd = 10,
BLD.sd = 100, CRFVOL.sd = 5, se.prop = TRUE)
{
if (any(ORCDRC[!is.na(ORCDRC)] < 0) | any(BLD[!is.na(BLD)] <
0) | any(CRFVOL[!is.na(CRFVOL)] < 0)) {
warning("Negative values for 'ORCDRC', 'BLD', 'CRFVOL' found")
}
OCSKG <- ORCDRC/1000 * HSIZE/100 * BLD * (100 - CRFVOL)/100
if (se.prop == TRUE) {
if (any(ORCDRC.sd[!is.na(ORCDRC.sd)] < 0)) {
ORCDRC.sd = ifelse(is.na(ORCDRC.sd) | ORCDRC.sd <
0, 0, ORCDRC.sd)
warning("Replacing negative values for 'ORCDRC.sd'")
}
if (any(BLD.sd[!is.na(BLD.sd)] < 0)) {
BLD.sd = ifelse(is.na(BLD.sd) | BLD.sd < 0, 0, BLD.sd)
warning("Replacing negative values for 'BLD.sd'")
}
if (any(CRFVOL.sd[!is.na(CRFVOL.sd)] < 0)) {
CRFVOL.sd = ifelse(is.na(CRFVOL.sd) | CRFVOL.sd <
0, 0, CRFVOL.sd)
warning("Replacing negative values for 'CRFVOL.sd'")
}
OCSKG.sd <- 1e-07 * HSIZE * sqrt(BLD^2 * (100 - CRFVOL)^2 *
ORCDRC.sd^2 + ORCDRC^2 * (100 - CRFVOL)^2 * BLD.sd^2 +
ORCDRC^2 * BLD^2 * CRFVOL.sd^2)
attr(OCSKG, "measurementError") <- signif(OCSKG.sd,
3)
attr(OCSKG, "units") <- "kilograms per square-meter"
}
return(OCSKG)
}
OCSKGM <- OCSKGM(ORCDRC = dat$SOC*10, BLD = dat$BLD*1000, CRFVOL = dat$CRF,
HSIZE = 30)
# Convert Organic Carbon Stock from kg/m3 to t/ha
dat$OCS <- OCSKGM*10
# Explore calculated SOC stocks
summary(dat$OCS)
hist(dat$OCS, breaks = 50)
#Remove Nas
dat <- dat[complete.cases(dat$OCS),]
# Check if log-transformation improves the data distribution
hist(log(dat$OCS), breaks = 50)
# Add a new column for log-transformes carbon stocks
dat$clay <- log(dat$OCS)
# Save the final table in a .csv file
write.csv(dat, "02-Outputs/dataproc.csv", row.names = FALSE)
############ Splitting the dataset in calibraition (training the model) and validation (testing the model) ##############
library(caret)
# Define the random numbers table (to get reproducible result)
set.seed(26522)
# Create random selection of 75% of the data as 'train' dataset and 25% as 'test' dataset
train.ind <- createDataPartition(1:nrow(dat), p = .75, list = FALSE)
train <- dat[ train.ind,]
test  <- dat[-train.ind,]
# Check if both 'train' and 'test datasets' have similar distributions
summary(train$OCS)
summary(test$OCS)
plot(density (train$clay), col='red',
main='Statistical distribution of train and test datasets')
lines(density(test$clay), col='blue')
legend('topright', legend=c("train", "test"),
col=c("red", "blue"), lty=1, cex=1.5)
# Save the 'train' and 'test' datasets as .csv tables
write.csv(train, file="02-Outputs/SOC_dat_train.csv", row.names = FALSE)
write.csv(test, file="02-Outputs/SOC_dat_test.csv", row.names = FALSE)
# Clay ----
# Explore and clean the clay data
summary(data$clay)
dat <- data[complete.cases(data$clay),] # remove NA values
dat <- dat[dat$clay>0,] # remove 0 values
# Explore clay data, identify outliers
summary(dat$clay)
hist(dat$clay, breaks=100)
boxplot(dat$clay, horizontal=TRUE)
# Remove outliers automatically, using boxplot
out <- boxplot(dat$clay, horizontal=TRUE)$out
dat <- dat[!(dat$clay %in% out),]
################################ Calculating clay stocks for standard topsoil depth 0-30cm #######################
dat <-
dat[, c("id","top","bottom","clay","BLD","CRF","X",  "Y", "soil")]
#equal area spline clay
clay <-mpspline(dat, 'clay', d = c(0,30))
temp <- data.frame()
for (i in names(clay)){
t2 <- cbind(clay[[i]]$id,clay[[i]]$est_dcm[1])
temp <- rbind(temp,t2)
}
clay <- temp
##Since we are predicting clay we don't need Profiles without clay data
# keed only unique ids
dat <-dat[,c("id","X",  "Y", "soil") ]
dat <-dat[!duplicated(dat$id),]
#Rename columns of mpspline output
colnames(clay) <- c('id', 'clay')
#Merge the harmonized clay output with the data
dat <- merge(clay, dat[,c("id","X",  "Y", "soil") ], by='id', all.x=T)
# We are mapping clay therefore we exclude rows with NA for the clay column
dat <- dat[complete.cases(dat$clay),]
dat <- dat[dat$clay>0,]
#Finally merge the other attributes and set to numeric
dat$clay <- as.numeric(dat$clay)
############ Splitting the dataset in calibraition (training the model) and validation (testing the model) ##############
# Create random selection of 75% of the data as 'train' dataset and 25% as 'test' dataset
train.ind <- createDataPartition(1:nrow(dat), p = .75, list = FALSE)
train <- dat[ train.ind,]
test  <- dat[-train.ind,]
# Check if both 'train' and 'test datasets' have similar distributions
summary(train$clay)
summary(test$clay)
plot(density (train$clay), col='red',
main='Statistical distribution of train and test datasets')
lines(density(test$clay), col='blue')
legend('topright', legend=c("train", "test"),
col=c("red", "blue"), lty=1, cex=1.5)
# Save the 'train' and 'test' datasets as .csv tables
write.csv(train, file="02-Outputs/clay_dat_train.csv", row.names = FALSE)
write.csv(test, file="02-Outputs/clay_dat_test.csv", row.names = FALSE)
# pH ----
# Explore and clean the clay data
summary(data$pH)
dat <- data[complete.cases(data$pH),] # remove NA values
# Explore pH data, identify outliers
summary(dat$pH)
hist(dat$pH, breaks=100)
boxplot(dat$pH, horizontal=TRUE)
# Remove outliers automatically, using boxplot
out <- boxplot(dat$pH, horizontal=TRUE)$out
dat <- dat[!(dat$pH %in% out),]
################################ Calculating pH stocks for standard topsoil depth 0-30cm #######################
dat <-
dat[, c("id","top","bottom","pH","BLD","CRF","X",  "Y", "soil")]
#equal area spline pH
pH <-mpspline(dat, 'pH', d = c(0,30))
temp <- data.frame()
for (i in names(pH)){
t2 <- cbind(pH[[i]]$id,pH[[i]]$est_dcm[1])
temp <- rbind(temp,t2)
}
pH <- temp
##Since we are predicting pH we don't need Profiles without pH data
# keed only unique ids
dat <-dat[,c("id","X",  "Y", "soil") ]
dat <-dat[!duplicated(dat$id),]
#Rename columns of mpspline output
colnames(pH) <- c('id', 'pH')
#Merge the harmonized pH output with the data
dat <- merge(pH, dat[,c("id","X",  "Y", "soil") ], by='id', all.x=T)
# We are mapping pH therefore we exclude rows with NA for the pH column
dat <- dat[complete.cases(dat$pH),]
dat <- dat[dat$pH>0,]
#Finally merge the other attributes and set to numeric
dat$pH <- as.numeric(dat$pH)
############ Splitting the dataset in calibraition (training the model) and validation (testing the model) ##############
# Create random selection of 75% of the data as 'train' dataset and 25% as 'test' dataset
train.ind <- createDataPartition(1:nrow(dat), p = .75, list = FALSE)
train <- dat[ train.ind,]
test  <- dat[-train.ind,]
# Check if both 'train' and 'test datasets' have similar distributions
summary(train$pH)
summary(test$pH)
plot(density (train$pH), col='red',
main='Statistical distribution of train and test datasets')
lines(density(test$pH), col='blue')
legend('topright', legend=c("train", "test"),
col=c("red", "blue"), lty=1, cex=1.5)
# Save the 'train' and 'test' datasets as .csv tables
write.csv(train, file="02-Outputs/pH_dat_train.csv", row.names = FALSE)
write.csv(test, file="02-Outputs/pH_dat_test.csv", row.names = FALSE)
#######################################################
#
#  Select covariates
#  based on PCA and Correlation
#
# GSP-Secretariat
# Contact: Isabel.Luotto@fao.org
#
#######################################################
#Empty environment and cache ----
rm(list = ls());
gc()
#######################################################
#
#  User defined variables:
# Working directory
wd <- 'C:/Users/luottoi/Documents/GitHub/Digital-Soil-Mapping'
#List of soil attributes prepared in script #2
soilatt <- c('SOC','clay', 'pH')
#
#
#######################################################
# Set working directory
setwd(wd)
# load packages
library(sf)
library(terra)
library(data.table)
# Selection based on correlation ----
# list all .tif files the 'covs' folder and load them in a raster stack.
# all rasters should have same extent, resoultion and coordinate system
files <- list.files(path = "01-Data/covs", pattern = "tif*$", full.names = TRUE)
covs <- stack(files)
# Explore the data
names(covs)
i='SOC'
# Load the processed data for digital soil mapping.
# This table was prepared in the 'data_preparation_profiles' script
dat <- fread(paste0("02-Outputs/",i,"_dat_train.csv"))
#Use coordinates to turn point data into a vector with coordinates
# WGS84
projcrs <- "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"
dat <- st_as_sf(dat,
coords = c("X", "Y"),
crs = projcrs)
# Check that the points overlay with the rasters
plot(covs$avtr)
points(dat)
points(dat)
plots(dat)
plot(dat)
# Check that the points overlay with the rasters
plot(covs$avtr)
points(dat$id)
points(dat$geometry)
points(st_geometry(dat))
points(dat$id)
# Check that the points overlay with the rasters
plot(covs$avtr)
points(dat$id)
?points
points(vect(dat))
# Extract values from covariates to the soil points
dat <- extract(x = covs, y = dat, sp = TRUE)
summary(dat)
# Remove NA values
dat<-as.data.frame(dat)
dat <- dat[complete.cases(dat),]
# Test correlation between each covariate and the soil attribute
names(dat)
dat[,i]
test_covs <- cor(x = as.matrix(dat[,i]),
y = as.matrix(dat[,names(covs)]))
test_covs
x <- subset(melt(test_covs), value != 1 | complete.cases(test_covs))
x <- x[with(x, order(-abs(x$value))),]
(x <- subset(x,abs(x$value)>0.25))
selection <- as.character(x$X2)
x <- subset(melt(test_covs), value != 1 | complete.cases(test_covs))
x <- x[with(x, order(-abs(x$value))),]
(x <- subset(x,abs(x$value)>0.20))
selection <- as.character(x$X2)
# Leave only selected covariates in the 'covs' raster stack
covs <- covs[[selection]]
selection <- as.character(x$X2)
x <- subset(melt(test_covs), value != 1 | complete.cases(test_covs))
x <- x[with(x, order(-abs(x$value))),]
(x <- subset(x,abs(x$value)>0.20))
selection <- as.character(x$X2)
selection <- as.character(x$Var2)
# Leave only selected covariates in the 'covs' raster stack
covs <- covs[[selection]]
names(covs)
for (i in unique(soilatt)){
# Load the processed data for digital soil mapping.
# This table was prepared in the 'data_preparation_profiles' script
dat <- fread(paste0("02-Outputs/",i,"_dat_train.csv"))
#Use coordinates to turn point data into a vector with coordinates
# WGS84
projcrs <- "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"
dat <- st_as_sf(dat,
coords = c("X", "Y"),
crs = projcrs)
# Check that the points overlay with the rasters
plot(covs$avtr)
points(vect(dat))
# Extract values from covariates to the soil points
dat <- extract(x = covs, y = dat, sp = TRUE)
summary(dat)
# Remove NA values
dat<-as.data.frame(dat)
dat <- dat[complete.cases(dat),]
# Test correlation between each covariate and the soil attribute
names(dat)
test_covs <- cor(x = as.matrix(dat[,i]),
y = as.matrix(dat[,names(covs)]))
test_covs
# Select only the covariates that have correlation higher than 0.3
library(reshape)
x <- subset(melt(test_covs), value != 1 | complete.cases(test_covs))
x <- x[with(x, order(-abs(x$value))),]
(x <- subset(x,abs(x$value)>0.20))
selection <- as.character(x$Var2)
# Leave only selected covariates in the 'covs' raster stack
covs <- covs[[selection]]
names(covs)
save(covs, file = paste0("02-Outputs/", i,"_covariates.RData"))
}
# Load the processed data for digital soil mapping.
# This table was prepared in the 'data_preparation_profiles' script
dat <- fread(paste0("02-Outputs/",i,"_dat_train.csv"))
#Use coordinates to turn point data into a vector with coordinates
# WGS84
projcrs <- "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"
dat <- st_as_sf(dat,
coords = c("X", "Y"),
crs = projcrs)
# Check that the points overlay with the rasters
plot(covs$avtr)
# Extract values from covariates to the soil points
dat <- extract(x = covs, y = dat, sp = TRUE)
summary(dat)
# Remove NA values
dat<-as.data.frame(dat)
dat <- dat[complete.cases(dat),]
# Test correlation between each covariate and the soil attribute
names(dat)
test_covs <- cor(x = as.matrix(dat[,i]),
y = as.matrix(dat[,names(covs)]))
test_covs
# Select only the covariates that have correlation higher than 0.3
library(reshape)
library(reshape)
x <- subset(melt(test_covs), value != 1 | complete.cases(test_covs))
x <- x[with(x, order(-abs(x$value))),]
(x <- subset(x,abs(x$value)>0.20))
selection <- as.character(x$Var2)
# Leave only selected covariates in the 'covs' raster stack
covs <- covs[[selection]]
names(covs)
save(covs, file = paste0("02-Outputs/", i,"_covariates.RData"))
i ='pH'
# Load the processed data for digital soil mapping.
# This table was prepared in the 'data_preparation_profiles' script
dat <- fread(paste0("02-Outputs/",i,"_dat_train.csv"))
#Use coordinates to turn point data into a vector with coordinates
# WGS84
projcrs <- "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"
dat <- st_as_sf(dat,
coords = c("X", "Y"),
crs = projcrs)
# Check that the points overlay with the rasters
plot(covs$avtr)
points(vect(dat))
# Extract values from covariates to the soil points
dat <- extract(x = covs, y = dat, sp = TRUE)
summary(dat)
# Remove NA values
dat<-as.data.frame(dat)
dat <- dat[complete.cases(dat),]
# Test correlation between each covariate and the soil attribute
names(dat)
test_covs <- cor(x = as.matrix(dat[,i]),
y = as.matrix(dat[,names(covs)]))
test_covs
x <- subset(melt(test_covs), value != 1 | complete.cases(test_covs))
x <- x[with(x, order(-abs(x$value))),]
(x <- subset(x,abs(x$value)>0.20))
selection <- as.character(x$Var2)
# Leave only selected covariates in the 'covs' raster stack
covs <- covs[[selection]]
selection <- as.character(x$X2)
# Leave only selected covariates in the 'covs' raster stack
covs <- covs[[selection]]
names(covs)
save(covs, file = paste0("02-Outputs/", i,"_covariates.RData"))
for (i in unique(soilatt)){
# Load the processed data for digital soil mapping.
# This table was prepared in the 'data_preparation_profiles' script
dat <- fread(paste0("02-Outputs/",i,"_dat_train.csv"))
#Use coordinates to turn point data into a vector with coordinates
# WGS84
projcrs <- "+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0"
dat <- st_as_sf(dat,
coords = c("X", "Y"),
crs = projcrs)
# Extract values from covariates to the soil points
dat <- extract(x = covs, y = dat, sp = TRUE)
summary(dat)
# Remove NA values
dat<-as.data.frame(dat)
dat <- dat[complete.cases(dat),]
# Test correlation between each covariate and the soil attribute
names(dat)
test_covs <- cor(x = as.matrix(dat[,i]),
y = as.matrix(dat[,names(covs)]))
test_covs
# Select only the covariates that have correlation higher than 0.3
x <- subset(melt(test_covs), value != 1 | complete.cases(test_covs))
x <- x[with(x, order(-abs(x$value))),]
(x <- subset(x,abs(x$value)>0.20))
selection <- as.character(x$X2)
# Leave only selected covariates in the 'covs' raster stack
covs <- covs[[selection]]
names(covs)
save(covs, file = paste0("02-Outputs/", i,"_covariates.RData"))
}
# Import land cover layer
landcover <- raster ('01-Data/land cover/LandCover.tif')
plot(landcover)
# Try to stack LandCover raster with the rest
covs <- stack(covs, landcover)
# Error indicates different extent. Check coordinate system (crs) of covs and LandCover
(covs@crs)
(landcover@crs)
# 'covs' has crs WGS 84, while landcover has crs UTM Zone 34
# We need to reproject the 'Lancover' raster using one of the 'covs' as a template
landcover <- projectRaster(from = landcover, to = covs$Terrain_attributes.1, method = "ngb")
# Save the reprojected raster
writeRaster(landcover, '02-Outputs/Landcover_WGS84.tif', overwrite=TRUE)
